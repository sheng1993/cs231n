{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.rnn_layers import *\n",
    "from cs231n.captioning_solver import CaptioningSolver\n",
    "from cs231n.classifiers.rnn import CaptioningRNN\n",
    "from cs231n.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions, sample_coco_minibatch_custom\n",
    "from cs231n.image_utils import image_from_url\n",
    "from cs231n.CaptioningModel import CaptioningModel\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BLEU_score(gt_caption, sample_caption):\n",
    "    \"\"\"\n",
    "    gt_caption: string, ground-truth caption\n",
    "    sample_caption: string, your model's predicted caption\n",
    "    Returns unigram BLEU score.\n",
    "    \"\"\"\n",
    "    import nltk\n",
    "    reference = [x for x in gt_caption.split(' ') \n",
    "                 if ('<END>' not in x and '<START>' not in x and '<UNK>' not in x)]\n",
    "    hypothesis = [x for x in sample_caption.split(' ') \n",
    "                  if ('<END>' not in x and '<START>' not in x and '<UNK>' not in x)]\n",
    "    BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, weights = [1])\n",
    "    return BLEUscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_image_idxs <class 'numpy.ndarray'> (400135,) int32\n",
      "word_to_idx <class 'dict'> 1004\n",
      "train_features <class 'numpy.ndarray'> (82783, 512) float32\n",
      "val_features <class 'numpy.ndarray'> (40504, 512) float32\n",
      "train_urls <class 'numpy.ndarray'> (82783,) <U63\n",
      "val_captions <class 'numpy.ndarray'> (195954, 17) int32\n",
      "train_captions <class 'numpy.ndarray'> (400135, 17) int32\n",
      "val_urls <class 'numpy.ndarray'> (40504,) <U63\n",
      "idx_to_word <class 'list'> 1004\n",
      "val_image_idxs <class 'numpy.ndarray'> (195954,) int32\n"
     ]
    }
   ],
   "source": [
    "# Load COCO data from disk; this returns a dictionary\n",
    "# We'll work with dimensionality-reduced features for this notebook, but feel\n",
    "# free to experiment with the original features by changing the flag below.\n",
    "data = load_coco_data(pca_features=True)\n",
    "\n",
    "# Print out all the keys and values from the data dictionary\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print(k, type(v), v.shape, v.dtype)\n",
    "    else:\n",
    "        print(k, type(v), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.random.seed(231)\n",
    "# small_data = load_coco_data(max_train=50)\n",
    "# sample_data = sample_coco_minibatch_custom(small_data, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capt_placeholder = tf.placeholder(tf.int32, [None, 17], name='capt_placeholder')\n",
    "# image_idxs_placeholder = tf.placeholder(tf.int32, [None], name='image_idxs_placeholder')\n",
    "# features_placeholder = tf.placeholder(tf.float32, data['train_features'].shape, name='features_placeholder')\n",
    "image_idx = tf.placeholder(tf.int32, shape=data['train_image_idxs'].shape)\n",
    "train_captions = tf.placeholder(tf.int32, shape=data['train_captions'].shape)\n",
    "train_features = tf.placeholder(tf.float32, shape=data['train_features'].shape)\n",
    "\n",
    "def _get_image_features(idxs, capts):\n",
    "    features = train_features[idxs]\n",
    "    return features, capts\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_idx, train_captions))\n",
    "dataset = dataset.map(_get_image_features)\n",
    "dataset = dataset.batch(128)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_batch = iterator.get_next();\n",
    "features, captions = next_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CaptioningModel(data['word_to_idx'], n_time_steps=16)\n",
    "loss = model.build_model()\n",
    "tf.get_variable_scope().reuse_variables()\n",
    "sample = model.build_sampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('training', reuse=tf.AUTO_REUSE):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 0\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "cuDNN launch failure : input shape ([128,1,1,512])\n\t [[Node: conv_featuresbatch_norm_2/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv_featuresbatch_norm_2/Reshape, conv_featuresbatch_norm/gamma/read, conv_featuresbatch_norm/beta/read, conv_featuresbatch_norm_2/Const, conv_featuresbatch_norm_2/Const)]]\n\t [[Node: truediv_1/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5499_truediv_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'conv_featuresbatch_norm_2/FusedBatchNorm', defined at:\n  File \"c:\\program files\\python35\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\program files\\python35\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\program files\\python35\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"c:\\program files\\python35\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"c:\\program files\\python35\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"c:\\program files\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\program files\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\program files\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\program files\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-e521b0d23998>\", line 2, in <module>\n    loss = model.build_model()\n  File \"E:\\workspace\\python\\cs231n_assignment3\\cs231n\\CaptioningModel.py\", line 89, in build_model\n    features = self._batch_norm(features, mode='train', name='conv_features')\n  File \"E:\\workspace\\python\\cs231n_assignment3\\cs231n\\CaptioningModel.py\", line 55, in _batch_norm\n    scope=(name+'batch_norm'))\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\", line 592, in batch_norm\n    scope=scope)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\", line 401, in _fused_batch_norm\n    _fused_batch_norm_inference)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\utils.py\", line 214, in smart_cond\n    return static_cond(pred_value, fn1, fn2)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\utils.py\", line 192, in static_cond\n    return fn1()\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\", line 388, in _fused_batch_norm_training\n    inputs, gamma, beta, epsilon=epsilon, data_format=data_format)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\", line 831, in fused_batch_norm\n    name=name)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 2033, in _fused_batch_norm\n    is_training=is_training, name=name)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): cuDNN launch failure : input shape ([128,1,1,512])\n\t [[Node: conv_featuresbatch_norm_2/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv_featuresbatch_norm_2/Reshape, conv_featuresbatch_norm/gamma/read, conv_featuresbatch_norm/beta/read, conv_featuresbatch_norm_2/Const, conv_featuresbatch_norm_2/Const)]]\n\t [[Node: truediv_1/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5499_truediv_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: cuDNN launch failure : input shape ([128,1,1,512])\n\t [[Node: conv_featuresbatch_norm_2/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv_featuresbatch_norm_2/Reshape, conv_featuresbatch_norm/gamma/read, conv_featuresbatch_norm/beta/read, conv_featuresbatch_norm_2/Const, conv_featuresbatch_norm_2/Const)]]\n\t [[Node: truediv_1/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5499_truediv_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b34712ea2d67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                 \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: cuDNN launch failure : input shape ([128,1,1,512])\n\t [[Node: conv_featuresbatch_norm_2/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv_featuresbatch_norm_2/Reshape, conv_featuresbatch_norm/gamma/read, conv_featuresbatch_norm/beta/read, conv_featuresbatch_norm_2/Const, conv_featuresbatch_norm_2/Const)]]\n\t [[Node: truediv_1/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5499_truediv_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'conv_featuresbatch_norm_2/FusedBatchNorm', defined at:\n  File \"c:\\program files\\python35\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\program files\\python35\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\program files\\python35\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"c:\\program files\\python35\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"c:\\program files\\python35\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"c:\\program files\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\program files\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\program files\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\program files\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-e521b0d23998>\", line 2, in <module>\n    loss = model.build_model()\n  File \"E:\\workspace\\python\\cs231n_assignment3\\cs231n\\CaptioningModel.py\", line 89, in build_model\n    features = self._batch_norm(features, mode='train', name='conv_features')\n  File \"E:\\workspace\\python\\cs231n_assignment3\\cs231n\\CaptioningModel.py\", line 55, in _batch_norm\n    scope=(name+'batch_norm'))\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\", line 592, in batch_norm\n    scope=scope)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\", line 401, in _fused_batch_norm\n    _fused_batch_norm_inference)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\utils.py\", line 214, in smart_cond\n    return static_cond(pred_value, fn1, fn2)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\utils.py\", line 192, in static_cond\n    return fn1()\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\", line 388, in _fused_batch_norm_training\n    inputs, gamma, beta, epsilon=epsilon, data_format=data_format)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\", line 831, in fused_batch_norm\n    name=name)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 2033, in _fused_batch_norm\n    is_training=is_training, name=name)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): cuDNN launch failure : input shape ([128,1,1,512])\n\t [[Node: conv_featuresbatch_norm_2/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv_featuresbatch_norm_2/Reshape, conv_featuresbatch_norm/gamma/read, conv_featuresbatch_norm/beta/read, conv_featuresbatch_norm_2/Const, conv_featuresbatch_norm_2/Const)]]\n\t [[Node: truediv_1/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5499_truediv_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Variables initializer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    feed_dict = {\n",
    "        image_idx: data['train_image_idxs'],\n",
    "        train_captions: data['train_captions'],\n",
    "        train_features: data['train_features']\n",
    "    }   \n",
    "    \n",
    "    # c = sess.run(sample, feed_dict={model.features:data['train_features'][sample_data[1]], model.captions: sample_data[0]})    \n",
    "    # print(decode_captions(c[0], data['idx_to_word']))\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(5):\n",
    "        print('Running epoch: %d' % i)\n",
    "        \n",
    "        sess.run(iterator.initializer, feed_dict=feed_dict)\n",
    "        while True:\n",
    "            try:\n",
    "                f, c = sess.run([features, captions])\n",
    "                _, l = sess.run([train_step, loss], feed_dict={model.features:f, model.captions: c})\n",
    "                count += 1\n",
    "                if count % 1000 == 0:\n",
    "                    print('loss: %f' % l)\n",
    "                    count = 0\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break;\n",
    "                \n",
    "    # c = sess.run(sample, feed_dict={model.features:data['train_features'][sample_data[1]], model.captions: sample_data[0]})    \n",
    "    # print(decode_captions(c[0], data['idx_to_word']))\n",
    "    \n",
    "    c = sess.run(sample, feed_dict={model.features:data['val_features'][data['val_image_idxs'][:128]]})\n",
    "    _c = sess.run(sample, feed_dict={model.features:data['train_features'][data['train_image_idxs'][:128]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gt: <START> a little boy wearing sunglasses laying on a shelf in a <UNK> <END>\n",
      "Sample: a white refrigerator <UNK> sitting in a kitchen <END>\n",
      "Gt: <START> man on bike <UNK> green vehicle on road <END>\n",
      "Sample: a wooden <UNK> <UNK> with a <UNK> <UNK> <UNK> <END>\n",
      "Gt: <START> a little boy wearing sunglasses laying on a shelf in a <UNK> <END>\n",
      "Sample: a white refrigerator <UNK> sitting in a kitchen <END>\n",
      "Gt: <START> man on bike <UNK> green vehicle on road <END>\n",
      "Sample: a wooden <UNK> <UNK> with a <UNK> <UNK> <UNK> <END>\n",
      "Gt: <START> two old toilets and an old sink in a small room <END>\n",
      "Sample: a clock on a pole near a tree <END>\n",
      "Gt: <START> two old toilets and an old sink in a small room <END>\n",
      "Sample: a clock on a pole near a tree <END>\n",
      "Gt: <START> a little boy wearing sunglasses laying on a shelf in a <UNK> <END>\n",
      "Sample: a white refrigerator <UNK> sitting in a kitchen <END>\n",
      "Gt: <START> a little boy wearing sunglasses laying on a shelf in a <UNK> <END>\n",
      "Sample: a white refrigerator <UNK> sitting in a kitchen <END>\n",
      "Gt: <START> an airplane with the landing gear <UNK> flying in the sky <END>\n",
      "Sample: a group of people that are standing in the sand <END>\n",
      "Gt: <START> a dog <UNK> out the open window of a car <END>\n",
      "Sample: a <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <END>\n",
      "Gt: <START> a close up of a motorcycle parked near a building <END>\n",
      "Sample: a kitchen with a stove cabinets and a refrigerator <END>\n",
      "Gt: <START> room with several toilets no roof and <UNK> coming through window <END>\n",
      "Sample: a kitchen with a refrigerator and a stove <END>\n",
      "Gt: <START> a small motorcycle standing alone on the <UNK> <END>\n",
      "Sample: a kitchen with a stove and a refrigerator <END>\n",
      "Gt: <START> a woman riding a motorcycle in front of a wall with an image of people <END>\n",
      "Sample: a couple of people that are on a court <END>\n",
      "Gt: <START> an airplane with the landing gear <UNK> flying in the sky <END>\n",
      "Sample: a group of people that are standing in the sand <END>\n",
      "Gt: <START> a group of <UNK> stand around a kitchen island <END>\n",
      "Sample: a kitchen with a stove top oven and a refrigerator <END>\n",
      "Gt: <START> a silver and red bus on street next to people and buildings <END>\n",
      "Sample: a man standing on top of a sandy beach <END>\n",
      "Gt: <START> a white toilet sitting next to a shower with a brown <UNK> <END>\n",
      "Sample: a <UNK> <UNK> <UNK> in a <UNK> <UNK> <END>\n",
      "Gt: <START> a city intersection with pedestrians a <UNK> and <UNK> traffic <END>\n",
      "Sample: a man standing by a tree with a <UNK> in his arms <END>\n",
      "Gt: <START> a air <UNK> with <UNK> and mountains in the back ground <END>\n",
      "Sample: a kitchen with a white fridge and a white <UNK> <END>\n",
      "Gt: <START> room with several toilets no roof and <UNK> coming through window <END>\n",
      "Sample: a kitchen with a refrigerator and a stove <END>\n",
      "Gt: <START> a yellow double decker bus sits in the snow lot <END>\n",
      "Sample: a cat sitting on a window <UNK> on a sunny day <END>\n",
      "Gt: <START> a <UNK> plane flying in a <UNK> sky <UNK> clouds <END>\n",
      "Sample: a man walking past a store with a clock on it <END>\n",
      "Gt: <START> a kitchen with a microwave a stove and <UNK> <END>\n",
      "Sample: a kitchen with a stove and a refrigerator <END>\n",
      "Gt: <START> stainless steel appliances in a <UNK> stone and <UNK> kitchen <END>\n",
      "Sample: a man sitting at a bar with a wine glass <END>\n",
      "Gt: <START> a group of three men stand in front of a vintage airplane <END>\n",
      "Sample: a <UNK> is <UNK> a <UNK> horse in a field <END>\n",
      "Gt: <START> a black gray and white cat sitting on a toilet seat <END>\n",
      "Sample: a man who is looking at a clock on a street <END>\n",
      "Gt: <START> a dog <UNK> out the open window of a car <END>\n",
      "Sample: a <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <END>\n",
      "Gt: <START> a very nice looking dining table by a bright window <END>\n",
      "Sample: a group of people sitting at a table with wine glasses <END>\n",
      "Gt: <START> giraffe with a very long neck standing alone <END>\n",
      "Sample: a white <UNK> <UNK> in a small white kitchen <END>\n",
      "Gt: <START> a white and red bus is going down the street <END>\n",
      "Sample: a kitchen with a refrigerator stove and a refrigerator <END>\n",
      "Gt: <START> an airplane with the landing gear <UNK> flying in the sky <END>\n",
      "Sample: a group of people that are standing in the sand <END>\n",
      "Gt: <START> three picnic tables with umbrellas sitting beside a body of water <END>\n",
      "Sample: a <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <END>\n",
      "Gt: <START> a dog <UNK> out the open window of a car <END>\n",
      "Sample: a <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <END>\n",
      "Gt: <START> a young man sits on a ledge resting his foot on a skateboard <END>\n",
      "Sample: a red <UNK> is in a dark <UNK> <END>\n",
      "Gt: <START> the <UNK> of two wooden benches in front of flowers and a tree <END>\n",
      "Sample: a man standing in a kitchen with a white refrigerator <END>\n",
      "Gt: <START> a group of people rest along a wall and watch a <UNK> <END>\n",
      "Sample: a <UNK> <UNK> a <UNK> to a <UNK> to <UNK> a <UNK> <UNK> <END>\n",
      "Gt: <START> motorcycles parked along a street and some <UNK> riding by <END>\n",
      "Sample: a <UNK> <UNK> with a <UNK> of <UNK> <UNK> in it <END>\n",
      "Gt: <START> a person with an umbrella standing by a building in the rain <END>\n",
      "Sample: a <UNK> of people walking down a sidewalk next to a building <END>\n",
      "Gt: <START> large group of cows on a street with bicycles and motorcycles <END>\n",
      "Sample: a refrigerator in a kitchen with a <UNK> on it <END>\n",
      "Gt: <START> man on bike <UNK> green vehicle on road <END>\n",
      "Sample: a wooden <UNK> <UNK> with a <UNK> <UNK> <UNK> <END>\n",
      "Gt: <START> a woman riding a motorcycle in front of a wall with an image of people <END>\n",
      "Sample: a couple of people that are on a court <END>\n",
      "Gt: <START> a bathroom with a shower and toilet in it <END>\n",
      "Sample: a man sitting at a table with a glass of wine <END>\n",
      "Gt: <START> a yellow and red <UNK> airplane in some grass <END>\n",
      "Sample: a kitchen with a stainless steel refrigerator and stove <END>\n",
      "Gt: <START> a group of three men stand in front of a vintage airplane <END>\n",
      "Sample: a <UNK> is <UNK> a <UNK> horse in a field <END>\n",
      "Gt: <START> a black gray and white cat sitting on a toilet seat <END>\n",
      "Sample: a man who is looking at a clock on a street <END>\n",
      "Gt: <START> a toilet sits in a tiled bathroom as sun <UNK> through the window <END>\n",
      "Sample: a white van parked in front of a white house <END>\n",
      "Gt: <START> a black table and a white toilet in a bathroom <END>\n",
      "Sample: a kitchen with a refrigerator and a stove <END>\n",
      "Gt: <START> a air <UNK> with <UNK> and mountains in the back ground <END>\n",
      "Sample: a kitchen with a white fridge and a white <UNK> <END>\n",
      "Gt: <START> a close up of a street parking meter with a car in the background <END>\n",
      "Sample: a black cat sitting on a blue chair with a blue <UNK> <END>\n",
      "Gt: <START> a air <UNK> with <UNK> and mountains in the back ground <END>\n",
      "Sample: a kitchen with a white fridge and a white <UNK> <END>\n",
      "Gt: <START> a kitchen with a microwave a stove and <UNK> <END>\n",
      "Sample: a kitchen with a stove and a refrigerator <END>\n",
      "Gt: <START> a person with an umbrella standing by a building in the rain <END>\n",
      "Sample: a <UNK> of people walking down a sidewalk next to a building <END>\n",
      "Gt: <START> a bathroom with a toilet and tiled floor <END>\n",
      "Sample: a <UNK> <UNK> refrigerator is <UNK> with a <UNK> <UNK> <END>\n",
      "Gt: <START> a man riding a skateboard over a set of steps <END>\n",
      "Sample: a small piece of bread is on a piece of paper <END>\n",
      "Gt: <START> a yellow double decker bus sits in the snow lot <END>\n",
      "Sample: a cat sitting on a window <UNK> on a sunny day <END>\n",
      "Gt: <START> an <UNK> bathroom has a toilet and <UNK> <END>\n",
      "Sample: a man standing in a kitchen with a refrigerator <END>\n",
      "Gt: <START> two people riding on a jet ski in a body of water <END>\n",
      "Sample: a man standing on a beach flying a red <UNK> <END>\n",
      "Gt: <START> a dirty bathroom has a toilet and broken tub in it <END>\n",
      "Sample: a kitchen with a stove and a refrigerator <END>\n",
      "Gt: <START> a group of old toilet bowls in a room <END>\n",
      "Sample: a girl is holding a bottle of wine in her hand <END>\n",
      "Gt: <START> a <UNK> bathroom with all its <UNK> broken and grass <UNK> on the ground <END>\n",
      "Sample: a big white bear that is in the air <END>\n",
      "Gt: <START> a <UNK> of some <UNK> in a vehicle during the day <END>\n",
      "Sample: a man is holding a <UNK> and a <UNK> <END>\n",
      "Gt: <START> a person with an umbrella standing by a building in the rain <END>\n",
      "Sample: a <UNK> of people walking down a sidewalk next to a building <END>\n",
      "Gt: <START> a dog <UNK> out the open window of a car <END>\n",
      "Sample: a <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <END>\n",
      "Gt: <START> a desk area with two laptops and chairs <END>\n",
      "Sample: a small white fridge in a kitchen <END>\n",
      "Gt: <START> a group of people <UNK> an elephant in its enclosure <END>\n",
      "Sample: a <UNK> is sitting in the middle of a swing <END>\n",
      "Gt: <START> three picnic tables with umbrellas sitting beside a body of water <END>\n",
      "Sample: a <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <END>\n",
      "Gt: <START> a man with a white hat cooking in a kitchen <END>\n",
      "Sample: a <UNK> <UNK> is <UNK> a <UNK> <UNK> <END>\n",
      "Gt: <START> a bathroom with a shower and toilet in it <END>\n",
      "Sample: a man sitting at a table with a glass of wine <END>\n",
      "Gt: <START> a yellow double decker bus sits in the snow lot <END>\n",
      "Sample: a cat sitting on a window <UNK> on a sunny day <END>\n",
      "Gt: <START> a <UNK> bus parked on a lush green lawn <END>\n",
      "Sample: a man that is standing in the dirt with a bat <END>\n",
      "Gt: <START> the <UNK> of two wooden benches in front of flowers and a tree <END>\n",
      "Sample: a man standing in a kitchen with a white refrigerator <END>\n",
      "Gt: <START> a <UNK> bus parked on a lush green lawn <END>\n",
      "Sample: a man that is standing in the dirt with a bat <END>\n",
      "Gt: <START> a park is <UNK> with <UNK> bushes and two benches <END>\n",
      "Sample: a young man is <UNK> a <UNK> hair <END>\n",
      "Gt: <START> a woman riding a motorcycle in front of a wall with an image of people <END>\n",
      "Sample: a couple of people that are on a court <END>\n",
      "Gt: <START> a man and a woman in a <UNK> dress have an umbrella <END>\n",
      "Sample: a pair of scissors and a <UNK> on a <UNK> <END>\n",
      "Gt: <START> a woman riding a motorcycle in front of a wall with an image of people <END>\n",
      "Sample: a couple of people that are on a court <END>\n",
      "Gt: <START> the <UNK> of people and a horse is seen in the distance <END>\n",
      "Sample: a man stands in front of a refrigerator <END>\n",
      "Gt: <START> graffiti making <UNK> of the <UNK> on a fire hydrant <END>\n",
      "Sample: a kitchen with a refrigerator and a stove <END>\n",
      "Gt: <START> a group of men sit in bed reading their books <END>\n",
      "Sample: a <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <END>\n",
      "Gt: <START> a kitchen with white cabinets and stainless steel oven and <UNK> <END>\n",
      "Sample: a woman is drinking a glass of red wine <END>\n",
      "Gt: <START> a air <UNK> with <UNK> and mountains in the back ground <END>\n",
      "Sample: a kitchen with a white fridge and a white <UNK> <END>\n",
      "Gt: <START> room with several toilets no roof and <UNK> coming through window <END>\n",
      "Sample: a kitchen with a refrigerator and a stove <END>\n",
      "Gt: <START> a computer and keyboard on top of a table <END>\n",
      "Sample: a man and a woman are standing in the snow <END>\n",
      "Gt: <START> a small motorcycle standing alone on the <UNK> <END>\n",
      "Sample: a kitchen with a stove and a refrigerator <END>\n",
      "Gt: <START> the <UNK> of two wooden benches in front of flowers and a tree <END>\n",
      "Sample: a man standing in a kitchen with a white refrigerator <END>\n",
      "Gt: <START> a bathroom is being <UNK> and a person is painting the wall <END>\n",
      "Sample: a white teddy bear sitting on a black bench <END>\n",
      "Gt: <START> a group of <UNK> stand around a kitchen island <END>\n",
      "Sample: a kitchen with a stove top oven and a refrigerator <END>\n",
      "Gt: <START> an empty bench by a path with many buildings in the background <END>\n",
      "Sample: a <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <END>\n",
      "Gt: <START> trees in a field of grass with a sky background <END>\n",
      "Sample: a group of people sitting around a table with food on it <END>\n",
      "Gt: <START> an airplane with the landing gear <UNK> flying in the sky <END>\n",
      "Sample: a group of people that are standing in the sand <END>\n",
      "Gt: <START> an empty bench by a path with many buildings in the background <END>\n",
      "Sample: a <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <END>\n",
      "Gt: <START> a white and red bus is going down the street <END>\n",
      "Sample: a kitchen with a refrigerator stove and a refrigerator <END>\n",
      "Gt: <START> a yellow double decker bus sits in the snow lot <END>\n",
      "Sample: a cat sitting on a window <UNK> on a sunny day <END>\n",
      "Gt: <START> many people sitting at a park bench near trees <END>\n",
      "Sample: a person is <UNK> a glass of wine <END>\n",
      "Gt: <START> a group of three men stand in front of a vintage airplane <END>\n",
      "Sample: a <UNK> is <UNK> a <UNK> horse in a field <END>\n",
      "Gt: <START> a black gray and white cat sitting on a toilet seat <END>\n",
      "Sample: a man who is looking at a clock on a street <END>\n",
      "Gt: <START> a computer and keyboard on top of a table <END>\n",
      "Sample: a man and a woman are standing in the snow <END>\n",
      "Gt: <START> a dog looking out the <UNK> of a car <END>\n",
      "Sample: a cat that is laying down on a bed <END>\n",
      "Gt: <START> a desk area with two laptops and chairs <END>\n",
      "Sample: a small white fridge in a kitchen <END>\n"
     ]
    }
   ],
   "source": [
    "for gt_caption, sample_caption in zip(decode_captions(data['train_captions'][data['train_image_idxs'][:100]], data['idx_to_word']), \n",
    "                                      decode_captions(_c, data['idx_to_word'])):\n",
    "    print('Gt: %s' % gt_caption)\n",
    "    print('Sample: %s' % sample_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.201597\n"
     ]
    }
   ],
   "source": [
    "total_score = 0.0\n",
    "for gt_caption, sample_caption in zip(decode_captions(data['val_captions'][:1000], data['idx_to_word']), \n",
    "                                      decode_captions(c, data['idx_to_word'])):\n",
    "    total_score += BLEU_score(gt_caption, sample_caption)\n",
    "print('BLEU score: %f' % (total_score/len(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feet event scissors lone about stacked party party party plastic rest wire wire wire lying traveling candles candles pen computer', 'prepared dining prepared dining using is park bicycle ice lettuce mirror game himself himself square rain himself swings pink catch']\n",
      "['<START> half a <UNK> <UNK> out on the ocean <END>', '<START> a man standing on the side of a road with bags of luggage <END>']\n",
      "['half a <UNK> <UNK> out on the ocean <END>', 'a man standing on the side of a road with bags of luggage <END>']\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    feed_dict = {\n",
    "        model.captions: sample_data[0],\n",
    "        model.image_idxs: sample_data[1],\n",
    "        model.features: data['train_features'][sample_data[1]]\n",
    "    }\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    c = sess.run(sample, feed_dict=feed_dict)\n",
    "    print(decode_captions(c[:2], data['idx_to_word']))\n",
    "    for i in range(50):\n",
    "        # sess.run(iterator.initializer, feed_dict=feed_dict)\n",
    "        # while True:\n",
    "            try:\n",
    "                #_, l = sess.run([train_step, loss], feed_dict=feed_dict)\n",
    "                \n",
    "                #if i % 10 == 0: print(l)\n",
    "                _, l = sess.run([train_step, loss], feed_dict=feed_dict)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break;\n",
    "                \n",
    "    c = sess.run(sample, feed_dict=feed_dict)\n",
    "    print(decode_captions(sample_data[0][:2], data['idx_to_word']))\n",
    "    print(decode_captions(c[:2], data['idx_to_word']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
